#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Modify: 2024-04-03
# Repo: https://github.com/Cp0204/quark_auto_save
# ConfigFile: quark_config.json

import os
import re
import sys
import json
import time
import random
import asyncio
import aiohttp
import logging
from datetime import datetime
from functools import lru_cache
from typing import Dict, List, Any, Optional, Tuple, Union

# å…¼å®¹é’é¾™
try:
    from treelib.tree import Tree
except ImportError:
    os.system("pip3 install treelib aiohttp &> /dev/null")
    from treelib.tree import Tree

CONFIG_DATA: Dict[str, Any] = {}
NOTIFYS: List[str] = []
GH_PROXY = os.environ.get("GH_PROXY", "https://ghproxy.net/")

MAGIC_REGEX: Dict[str, Dict[str, str]] = {
    "$TV": {
        "pattern": ".*?(S\\d{1,2}E)?P?(\\d{1,3}).*?\\.(mp4|mkv)",
        "replace": "\\1\\2.\\3",
    },
}

# è®¾ç½®æ—¥å¿—é…ç½®
logger = logging.getLogger('QuarkAutoSave')
logger.setLevel(logging.DEBUG)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler('quark_save.log', mode='a', encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# åˆ›å»ºç»ˆç«¯å¤„ç†å™¨
stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.DEBUG)
stream_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
stream_handler.setFormatter(stream_formatter)

# å°†å¤„ç†å™¨æ·»åŠ åˆ°æ—¥å¿—è®°å½•å™¨
logger.addHandler(file_handler)
logger.addHandler(stream_handler)

async def fetch(session: aiohttp.ClientSession, method: str, url: str, **kwargs) -> Optional[Dict[str, Any]]:
    try:
        async with session.request(method, url, **kwargs) as response:
            response.raise_for_status()
            try:
                return await response.json()
            except aiohttp.ContentTypeError:
                # å¦‚æœå“åº”ä¸æ˜¯JSONï¼Œå°è¯•è¯»å–åŸå§‹æ–‡æœ¬
                text = await response.text()
                logger.error(f"å“åº”ä¸æ˜¯JSONæ ¼å¼: {method} {url} - å“åº”å†…å®¹: {text[:200]}")
                # å°è¯•è§£æä¸ºJSONï¼Œå³ä½¿Content-Typeä¸æ­£ç¡®
                try:
                    return json.loads(text)
                except json.JSONDecodeError:
                    # å¦‚æœä»ç„¶æ— æ³•è§£æä¸ºJSONï¼Œè¿”å›åŒ…å«é”™è¯¯ä¿¡æ¯çš„å­—å…¸
                    return {
                        "code": -1,
                        "message": f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {text[:100]}...",
                        "raw_response": text[:500]
                    }
            except json.JSONDecodeError as e:
                text = await response.text()
                logger.error(f"JSONè§£æé”™è¯¯: {method} {url} - é”™è¯¯: {e} - å“åº”å†…å®¹: {text[:200]}")
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                try:
                    # å°è¯•ä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é—®é¢˜
                    fixed_text = text.strip()
                    if not fixed_text.startswith('{') and not fixed_text.startswith('['):
                        # å¦‚æœä¸æ˜¯ä»¥{æˆ–[å¼€å¤´ï¼Œå°è¯•åŒ…è£…å®ƒ
                        # è½¬ä¹‰å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                        import json as json_module
                        escaped_text = json_module.dumps(fixed_text)
                        fixed_text = f'{{"data": {escaped_text}}}'
                    return json.loads(fixed_text)
                except json.JSONDecodeError:
                    return {
                        "code": -1,
                        "message": f"JSONè§£æå¤±è´¥: {str(e)}",
                        "raw_response": text[:500]
                    }
    except aiohttp.ClientResponseError as e:
        # ç®€åŒ–é”™è¯¯å¤„ç†ï¼Œä¸å†æ£€æŸ¥frå‚æ•°åˆ‡æ¢
        url_str = str(url)
        # å¦‚æœURLæ˜¯URLå¯¹è±¡ï¼Œæå–å®é™…çš„URLå­—ç¬¦ä¸²
        import re
        url_match = re.search(r"URL\('([^']+)'\)", url_str)
        if url_match:
            url_str = url_match.group(1)
        
        logger.error(f"è¯·æ±‚å¤±è´¥: {method} {url_str} - {e}")
        return {
            "code": e.status,
            "message": f"è¯·æ±‚å¤±è´¥: {method} {url_str} - {e}",
            "status": e.status
        }
    except Exception as e:
        logger.error(f"è¯·æ±‚å¤±è´¥: {method} {url} - {e}")
        # å¯¹äºéClientResponseErrorå¼‚å¸¸ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„å­—å…¸
        return {
            "code": -1,
            "message": f"è¯·æ±‚å¤±è´¥: {method} {url} - {e}",
            "status": -1
        }

def magic_regex_func(pattern: str, replace: str) -> Tuple[str, str]:
    keyword = pattern
    # æ£€æŸ¥CONFIG_DATAæ˜¯å¦å·²åˆå§‹åŒ–å¹¶ä¸”åŒ…å«magic_regex
    if CONFIG_DATA and "magic_regex" in CONFIG_DATA and keyword in CONFIG_DATA["magic_regex"]:
        pattern = CONFIG_DATA["magic_regex"][keyword]["pattern"]
        if replace == "":
            replace = CONFIG_DATA["magic_regex"][keyword]["replace"]
    return pattern, replace

def send_ql_notify(title: str, body: str, cookie_index: Optional[int] = None) -> None:
    try:
        import notify
        
        # ä»æ‰€æœ‰Cookieä¸­æŸ¥æ‰¾æœ‰æ•ˆçš„é’‰é’‰é€šçŸ¥é…ç½®
        dd_bot_token: Optional[str] = None
        dd_bot_secret: Optional[str] = None
        
        # ä»æ‰€æœ‰Cookieä¸­æŸ¥æ‰¾æœ‰æ•ˆçš„TGé€šçŸ¥é…ç½®
        tg_bot_token: Optional[str] = None
        tg_user_id: Optional[str] = None
        
        if CONFIG_DATA.get("cookies"):
            # å¦‚æœæŒ‡å®šäº†cookie_indexï¼Œä½¿ç”¨è¯¥ç´¢å¼•å¯¹åº”çš„cookieé…ç½®
            if cookie_index is not None and 0 <= cookie_index < len(CONFIG_DATA["cookies"]):
                cookie_config = CONFIG_DATA["cookies"][cookie_index]
                # æŸ¥æ‰¾é’‰é’‰é…ç½®
                token = cookie_config.get("dd_bot_token")
                secret = cookie_config.get("dd_bot_secret")
                if token and secret:
                    dd_bot_token = token
                    dd_bot_secret = secret
                
                # æŸ¥æ‰¾TGé…ç½®
                tg_token = cookie_config.get("tg_bot_token")
                tg_id = cookie_config.get("tg_user_id")
                if tg_token and tg_id:
                    tg_bot_token = tg_token
                    tg_user_id = tg_id
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å½“å‰cookieçš„é…ç½®ï¼Œå›é€€åˆ°éå†æ‰€æœ‰cookie
                    for cookie_config in CONFIG_DATA["cookies"]:
                        # æŸ¥æ‰¾é’‰é’‰é…ç½®
                        token = cookie_config.get("dd_bot_token")
                        secret = cookie_config.get("dd_bot_secret")
                        if token and secret:
                            dd_bot_token = token
                            dd_bot_secret = secret
                        
                        # æŸ¥æ‰¾TGé…ç½®
                        tg_token = cookie_config.get("tg_bot_token")
                        tg_id = cookie_config.get("tg_user_id")
                        if tg_token and tg_id:
                            tg_bot_token = tg_token
                            tg_user_id = tg_id
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šcookie_indexï¼Œéå†æ‰€æœ‰cookieï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                for cookie_config in CONFIG_DATA["cookies"]:
                    # æŸ¥æ‰¾é’‰é’‰é…ç½®
                    token = cookie_config.get("dd_bot_token")
                    secret = cookie_config.get("dd_bot_secret")
                    if token and secret:
                        dd_bot_token = token
                        dd_bot_secret = secret
                    
                    # æŸ¥æ‰¾TGé…ç½®
                    tg_token = cookie_config.get("tg_bot_token")
                    tg_id = cookie_config.get("tg_user_id")
                    if tg_token and tg_id:
                        tg_bot_token = tg_token
                        tg_user_id = tg_id
        
        # å¦‚æœæ‰¾åˆ°äº†é’‰é’‰é…ç½®ï¼Œå‘é€é’‰é’‰é€šçŸ¥
        if dd_bot_token and dd_bot_secret:
            # ä½¿ç”¨ignore_default_config=Trueç¡®ä¿åªå‘é€é’‰é’‰é€šçŸ¥
            notify.send(
                title, 
                body, 
                ignore_default_config=True,
                DD_BOT_TOKEN=dd_bot_token,
                DD_BOT_SECRET=dd_bot_secret,
                CONSOLE=True,
                HITOKOTO=False
            )
            logger.info("é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
        else:
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„é’‰é’‰é€šçŸ¥é…ç½®ï¼Œè·³è¿‡é’‰é’‰æ¨é€")
        
        # å¦‚æœæ‰¾åˆ°äº†TGé…ç½®ï¼Œå‘é€TGé€šçŸ¥
        if tg_bot_token and tg_user_id:
            # ä½¿ç”¨ignore_default_config=Trueç¡®ä¿åªå‘é€TGé€šçŸ¥
            notify.send(
                title, 
                body, 
                ignore_default_config=True,
                TG_BOT_TOKEN=tg_bot_token,
                TG_USER_ID=tg_user_id,
                CONSOLE=True,
                HITOKOTO=False
            )
            logger.info("TGé€šçŸ¥å‘é€æˆåŠŸ")
        else:
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„TGé€šçŸ¥é…ç½®ï¼Œè·³è¿‡TGæ¨é€")
            
    except Exception as e:
        logger.error(f"å‘é€é€šçŸ¥æ¶ˆæ¯å¤±è´¥: {e}")

def add_notify(text: str) -> str:
    global NOTIFYS
    NOTIFYS.append(text)
    logger.info(text)
    return text

def download_file_sync(url: str, save_path: str) -> bool:
    try:
        import requests
        response = requests.get(url)
        if response.status_code == 200:
            with open(save_path, "wb") as file:
                file.write(response.content)
            return True
        else:
            logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {url} - çŠ¶æ€ç  {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¼‚å¸¸: {url} - {e}")
        return False

def get_cookies(cookie_val: Union[str, List[str], None]) -> Union[List[str], bool]:
    if isinstance(cookie_val, list):
        return cookie_val
    elif cookie_val:
        if "\n" in cookie_val:
            return cookie_val.split("\n")
        else:
            return [cookie_val]
    else:
        return False

class Quark:
    def __init__(self, cookie: str, index: Optional[int] = None):
        self.cookie = cookie.strip()
        self.index = index + 1
        self.is_active = False
        self.nickname = ""
        self.st = self.match_st_form_cookie(cookie)
        self.mparam = self.match_mparam_form_cookie(cookie)
        self.savepath_fid = {"/": "0"}

    def match_st_form_cookie(self, cookie: str) -> str:
        # ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é… =stxxxxxx; æ ¼å¼
        # æ”¯æŒå¤šç§æ ¼å¼ï¼š=stxxxxxx; æˆ– =stxxxxxxï¼ˆæ²¡æœ‰åˆ†å·ï¼‰
        # å¤¸å…‹Cookieä¸­stå‚æ•°é€šå¸¸ä»¥ =stxxxxxx å½¢å¼å‡ºç°
        match = re.search(r"=st([a-zA-Z0-9]+)[;]?", cookie)
        return match.group(1) if match else ""

    def match_mparam_form_cookie(self, cookie: str) -> Dict[str, str]:
        mparam = {}
        kps_match = re.search(r"(?<!\w)kps=([a-zA-Z0-9%]+)[;&]?", cookie)
        sign_match = re.search(r"(?<!\w)sign=([a-zA-Z0-9%]+)[;&]?", cookie)
        vcode_match = re.search(r"(?<!\w)vcode=([a-zA-Z0-9%]+)[;&]?", cookie)
        if kps_match and sign_match and vcode_match:
            mparam = {
                "kps": kps_match.group(1).replace("%25", "%"),
                "sign": sign_match.group(1).replace("%25", "%"),
                "vcode": vcode_match.group(1).replace("%25", "%"),
            }
        return mparam

    def common_headers(self) -> Dict[str, str]:
        headers = {
            "cookie": self.cookie,
            "content-type": "application/json",
        }
        if self.st:  # self.st ç°åœ¨æ˜¯å­—ç¬¦ä¸²ï¼Œç©ºå­—ç¬¦ä¸²ä¸º False
            headers["x-clouddrive-st"] = self.st
        return headers

    async def init(self, session: aiohttp.ClientSession) -> Union[Dict[str, Any], bool]:
        account_info = await self.get_account_info(session)
        if account_info:
            self.is_active = True
            self.nickname = account_info["nickname"]
            return account_info
        else:
            return False

    async def get_account_info(self, session: aiohttp.ClientSession) -> Union[Dict[str, Any], bool]:
        url = "https://pan.quark.cn/account/info"
        querystring = {"fr": "pc", "platform": "pc"}
        headers = self.common_headers()
        response = await fetch(session, "GET", url, headers=headers, params=querystring)
        if response and response.get("data"):
            return response["data"]
        else:
            return False

    async def get_growth_info(self, session: aiohttp.ClientSession) -> Union[Dict[str, Any], bool]:
        url = "https://drive-pc.quark.cn/1/clouddrive/capacity/growth/info"
        querystring = {
            "pr": "ucpro",
            "fr": "android",
            "kps": self.mparam.get("kps"),
            "sign": self.mparam.get("sign"),
            "vcode": self.mparam.get("vcode"),
        }
        headers = {
            "content-type": "application/json",
        }
        response = await fetch(session, "GET", url, headers=headers, params=querystring)
        if response and response.get("data"):
            return response["data"]
        else:
            return False

    async def get_growth_sign(self, session: aiohttp.ClientSession) -> Tuple[bool, Union[int, str]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/capacity/growth/sign"
        querystring = {
            "pr": "ucpro",
            "fr": "android",
            "kps": self.mparam.get("kps"),
            "sign": self.mparam.get("sign"),
            "vcode": self.mparam.get("vcode"),
        }
        payload = {
            "sign_cyclic": True,
        }
        headers = {
            "content-type": "application/json",
        }
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        if response and response.get("data"):
            return True, response["data"]["sign_daily_reward"]
        elif response:
            return False, response["message"]
        else:
            return False, "æœªçŸ¥é”™è¯¯"

    def get_id_from_url(self, url: str) -> Union[Tuple[str, str], None]:
        url = url.replace("https://pan.quark.cn/s/", "")
        pattern = r"(\w+)(#/list/share.*/(\w+))?"
        match = re.search(pattern, url)
        if match:
            pwd_id = match.group(1)
            if match.group(2):
                pdir_fid = match.group(3)
            else:
                pdir_fid = "0"
            return pwd_id, pdir_fid
        else:
            return None

    async def get_stoken(self, session: aiohttp.ClientSession, pwd_id: str) -> Tuple[bool, str]:
        url = "https://drive-pc.quark.cn/1/clouddrive/share/sharepage/token"
        querystring = {"pr": "ucpro", "fr": "pc"}
        payload = {"pwd_id": pwd_id, "passcode": ""}
        headers = self.common_headers()
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        if response:
            if response.get("data"):
                return True, response["data"]["stoken"]
            elif response.get("message"):
                # ç¡®ä¿æ¶ˆæ¯æ˜¯å­—ç¬¦ä¸²ä¸”ä¸åŒ…å«å¯èƒ½ç ´åJSONçš„å­—ç¬¦
                message = str(response["message"])
                # ç§»é™¤å¯èƒ½ç ´åJSONçš„ç‰¹æ®Šå­—ç¬¦
                message = message.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
                return False, message
            elif response.get("code") == -1:
                # fetchå‡½æ•°è¿”å›çš„éJSONå“åº”
                raw_response = response.get("raw_response", "")
                if raw_response:
                    # æˆªå–å‰100ä¸ªå­—ç¬¦ï¼Œé¿å…è¿‡é•¿
                    return False, f"APIè¿”å›éJSONå“åº”: {raw_response[:100]}..."
                else:
                    return False, "APIè¿”å›éJSONå“åº”"
            else:
                return False, "æœªçŸ¥APIå“åº”æ ¼å¼"
        else:
            return False, "è¯·æ±‚å¤±è´¥æˆ–æ— å“åº”"

    async def get_detail(self, session: aiohttp.ClientSession, pwd_id: str, stoken: str, pdir_fid: str) -> List[Dict[str, Any]]:
        file_list = []
        page = 1
        while True:
            url = "https://drive-pc.quark.cn/1/clouddrive/share/sharepage/detail"
            querystring = {
                "pr": "ucpro",
                "fr": "pc",
                "pwd_id": pwd_id,
                "stoken": stoken,
                "pdir_fid": pdir_fid,
                "force": "0",
                "_page": page,
                "_size": "50",
                "_fetch_banner": "0",
                "_fetch_share": "0",
                "_fetch_total": "1",
                "_sort": "file_type:asc,updated_at:desc",
            }
            headers = self.common_headers()
            response = await fetch(session, "GET", url, headers=headers, params=querystring)
            if response and response["data"]["list"]:
                file_list += response["data"]["list"]
                page += 1
            else:
                break
            if len(file_list) >= response["metadata"]["_total"]:
                break
        return file_list

    async def get_fids(self, session: aiohttp.ClientSession, file_paths: Tuple[str, ...]) -> List[Dict[str, Any]]:
        # ä½¿ç”¨å®ä¾‹çº§åˆ«çš„ç¼“å­˜ï¼Œé¿å…åç¨‹é‡ç”¨é—®é¢˜
        cache_key = tuple(file_paths)
        if not hasattr(self, '_fids_cache'):
            self._fids_cache = {}
        
        if cache_key in self._fids_cache:
            return self._fids_cache[cache_key]
        
        fids = []
        while file_paths:
            batch = file_paths[:50]
            file_paths = file_paths[50:]
            url = "https://drive-pc.quark.cn/1/clouddrive/file/info/path_list"
            querystring = {"pr": "ucpro", "fr": "pc"}
            payload = {"file_path": batch, "namespace": "0"}
            headers = self.common_headers()
            response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
            if response and response["code"] == 0:
                fids += response["data"]
            else:
                logger.error(f"è·å–ç›®å½•IDå¤±è´¥: {response['message'] if response else 'æ— å“åº”'}")
                break
        
        # ç¼“å­˜ç»“æœ
        self._fids_cache[cache_key] = fids
        return fids

    async def ls_dir(self, session: aiohttp.ClientSession, pdir_fid: str) -> List[Dict[str, Any]]:
        file_list = []
        page = 1
        while True:
            url = "https://drive-pc.quark.cn/1/clouddrive/file/sort"
            querystring = {
                "pr": "ucpro",
                "fr": "pc",
                "uc_param_str": "",
                "pdir_fid": pdir_fid,
                "_page": page,
                "_size": "50",
                "_fetch_total": "1",
                "_fetch_sub_dirs": "0",
                "_sort": "file_type:asc,updated_at:desc",
            }
            headers = self.common_headers()
            response = await fetch(session, "GET", url, headers=headers, params=querystring)
            if response and response["data"]["list"]:
                file_list += response["data"]["list"]
                page += 1
            else:
                break
            if len(file_list) >= response["metadata"]["_total"]:
                break
        return file_list

    async def save_file(self, session: aiohttp.ClientSession, fid_list: List[str], fid_token_list: List[str], to_pdir_fid: str, pwd_id: str, stoken: str) -> Optional[Dict[str, Any]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/share/sharepage/save"
        querystring = {
            "pr": "ucpro",
            "fr": "pc",
            "uc_param_str": "",
            "app": "clouddrive",
            "__dt": int(random.uniform(1, 5) * 60 * 1000),
            "__t": datetime.now().timestamp(),
        }
        payload = {
            "fid_list": fid_list,
            "fid_token_list": fid_token_list,
            "to_pdir_fid": to_pdir_fid,
            "pwd_id": pwd_id,
            "stoken": stoken,
            "pdir_fid": "0",
            "scene": "link",
        }
        headers = self.common_headers()
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        return response

    async def mkdir(self, session: aiohttp.ClientSession, dir_path: str) -> Optional[Dict[str, Any]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/file"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {
            "pdir_fid": "0",
            "file_name": "",
            "dir_path": dir_path,
            "dir_init_lock": False,
        }
        headers = self.common_headers()
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        return response

    async def rename(self, session: aiohttp.ClientSession, fid: str, file_name: str) -> Optional[Dict[str, Any]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/file/rename"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {"fid": fid, "file_name": file_name}
        headers = self.common_headers()
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        return response

    async def delete(self, session: aiohttp.ClientSession, filelist: List[str]) -> Optional[Dict[str, Any]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/file/delete"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {"action_type": 2, "filelist": filelist, "exclude_fids": []}
        headers = self.common_headers()
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        return response

    async def recycle_list(self, session: aiohttp.ClientSession, page: int = 1, size: int = 30) -> List[Dict[str, Any]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/file/recycle/list"
        querystring = {
            "_page": page,
            "_size": size,
            "pr": "ucpro",
            "fr": "pc",
            "uc_param_str": "",
        }
        headers = self.common_headers()
        response = await fetch(session, "GET", url, headers=headers, params=querystring)
        if response:
            return response["data"]["list"]
        else:
            return []

    async def recycle_remove(self, session: aiohttp.ClientSession, record_list: List[str]) -> Optional[Dict[str, Any]]:
        url = "https://drive-pc.quark.cn/1/clouddrive/file/recycle/remove"
        querystring = {"uc_param_str": "", "fr": "pc", "pr": "ucpro"}
        payload = {
            "select_mode": 2,
            "record_list": record_list,
        }
        headers = self.common_headers()
        response = await fetch(session, "POST", url, json=payload, headers=headers, params=querystring)
        return response

    async def update_savepath_fid(self, session: aiohttp.ClientSession, tasklist: List[Dict[str, Any]]) -> bool:
        dir_paths = [
            re.sub(r"/{2,}", "/", f"/{item['savepath']}")
            for item in tasklist
            if not item.get("enddate")
            or (
                datetime.now().date()
                <= datetime.strptime(item["enddate"], "%Y-%m-%d").date()
            )
        ]
        if not dir_paths:
            return False
        dir_paths_exist_arr = await self.get_fids(session, tuple(dir_paths))
        dir_paths_exist = [item["file_path"] for item in dir_paths_exist_arr]
        dir_paths_unexist = list(set(dir_paths) - set(dir_paths_exist) - set(["/"]))
        tasks = []
        for dir_path in dir_paths_unexist:
            tasks.append(self.mkdir(session, dir_path))
        mkdir_results = await asyncio.gather(*tasks)
        for dir_path, mkdir_return in zip(dir_paths_unexist, mkdir_results):
            if mkdir_return and mkdir_return.get("code") == 0:
                new_dir = mkdir_return["data"]
                dir_paths_exist_arr.append(
                    {"file_path": dir_path, "fid": new_dir["fid"]}
                )
                logger.info(f"åˆ›å»ºæ–‡ä»¶å¤¹ï¼š{dir_path}")
            else:
                logger.error(f"åˆ›å»ºæ–‡ä»¶å¤¹ï¼š{dir_path} å¤±è´¥, {mkdir_return['message'] if mkdir_return else 'æ— å“åº”'}")
        # å‚¨å­˜ç›®æ ‡ç›®å½•çš„fid
        for dir_path in dir_paths_exist_arr:
            self.savepath_fid[dir_path["file_path"]] = dir_path["fid"]
        return True

    async def do_save_check(self, session: aiohttp.ClientSession, shareurl: str, savepath: str) -> Union[Dict[str, Any], bool]:
        try:
            result = self.get_id_from_url(shareurl)
            if result is None:
                return False
            pwd_id, pdir_fid = result
            is_sharing, stoken = await self.get_stoken(session, pwd_id)
            if not is_sharing:
                add_notify(f"âŒï¼š{stoken}\n")
                return False
            share_file_list = await self.get_detail(session, pwd_id, stoken, pdir_fid)
            fid_list = [item["fid"] for item in share_file_list]
            fid_token_list = [item["share_fid_token"] for item in share_file_list]
            file_name_list = [item["file_name"] for item in share_file_list]
            if not fid_list:
                return False
            
            get_fids = await self.get_fids(session, (savepath,))
            to_pdir_fid = None
            if get_fids and len(get_fids) > 0:
                to_pdir_fid = get_fids[0]["fid"]
            else:
                mkdir_result = await self.mkdir(session, savepath)
                if mkdir_result and mkdir_result.get("data"):
                    to_pdir_fid = mkdir_result["data"]["fid"]
                else:
                    logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥: {savepath}")
                    return False
            
            save_file_return = await self.save_file(session, fid_list, fid_token_list, to_pdir_fid, pwd_id, stoken)
            if not save_file_return:
                return False
            if save_file_return["code"] == 41017:
                return False
            elif save_file_return["code"] == 0:
                dir_file_list = await self.ls_dir(session, to_pdir_fid)
                del_list = [
                    item["fid"]
                    for item in dir_file_list
                    if (item["file_name"] in file_name_list)
                    and ((datetime.now().timestamp() - item["created_at"]) < 60)
                ]
                if del_list:
                    await self.delete(session, del_list)
                    recycle_list = await self.recycle_list(session)
                    record_id_list = [
                        item["record_id"]
                        for item in recycle_list
                        if item["fid"] in del_list
                    ]
                    await self.recycle_remove(session, record_id_list)
                return save_file_return
            else:
                return False
        except Exception as e:
            if os.environ.get("DEBUG") == "True":
                logger.error(f"è½¬å­˜æµ‹è¯•å¤±è´¥: {str(e)}")
            return False

    async def do_save_task(self, session: aiohttp.ClientSession, task: Dict[str, Any]) -> Optional[bool]:
        if task.get("shareurl_ban"):
            logger.info(f"ã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}")
            return None

        result = self.get_id_from_url(task["shareurl"])
        if result is None:
            return None
        pwd_id, pdir_fid = result
        is_sharing, stoken = await self.get_stoken(session, pwd_id)
        if not is_sharing:
            add_notify(f"âŒã€Š{task['taskname']}ã€‹ï¼š{stoken}\n")
            task["shareurl_ban"] = stoken
            return
        updated_tree = await self.dir_check_and_save(session, task, pwd_id, stoken, pdir_fid)
        if updated_tree.size(1) > 0:
            add_notify(f"âœ…ã€Š{task['taskname']}ã€‹æ·»åŠ è¿½æ›´ï¼š\n{updated_tree}")
            return True
        else:
            logger.info(f"ä»»åŠ¡ç»“æŸï¼šæ²¡æœ‰æ–°çš„è½¬å­˜ä»»åŠ¡")
            return False

    async def dir_check_and_save(self, session: aiohttp.ClientSession, task: Dict[str, Any], pwd_id: str, stoken: str, pdir_fid: str = "", subdir_path: str = "") -> Tree:
        tree = Tree()
        tree.create_node(task["savepath"], pdir_fid)
        share_file_list = await self.get_detail(session, pwd_id, stoken, pdir_fid)

        if not share_file_list:
            if subdir_path == "":
                task["shareurl_ban"] = "åˆ†äº«ä¸ºç©ºï¼Œæ–‡ä»¶å·²è¢«åˆ†äº«è€…åˆ é™¤"
                add_notify(f"ã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}")
            return tree
        elif (
            len(share_file_list) == 1
            and share_file_list[0]["dir"]
            and subdir_path == ""
        ):
            logger.info("ğŸ§  è¯¥åˆ†äº«æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œè¯»å–æ–‡ä»¶å¤¹å†…åˆ—è¡¨")
            share_file_list = await self.get_detail(session, pwd_id, stoken, share_file_list[0]["fid"])

        savepath = re.sub(r"/{2,}", "/", f"/{task['savepath']}{subdir_path}")
        if not self.savepath_fid.get(savepath):
            get_fids = await self.get_fids(session, (savepath,))
            if get_fids:
                self.savepath_fid[savepath] = get_fids[0]["fid"]
            else:
                # å°è¯•åˆ›å»ºç›®å½•ï¼Œå°±åƒdo_save_checkæ–¹æ³•ä¸­é‚£æ ·
                logger.info(f"ç›®å½• {savepath} ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º...")
                mkdir_result = await self.mkdir(session, savepath)
                if mkdir_result and mkdir_result.get("code") == 0:
                    self.savepath_fid[savepath] = mkdir_result["data"]["fid"]
                    logger.info(f"âœ… æˆåŠŸåˆ›å»ºç›®å½• {savepath}ï¼Œfid: {mkdir_result['data']['fid']}")
                else:
                    logger.error(f"âŒ ç›®å½• {savepath} åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡è½¬å­˜")
                    return tree
        to_pdir_fid = self.savepath_fid[savepath]
        dir_file_list = await self.ls_dir(session, to_pdir_fid)

        need_save_list = []
        for share_file in share_file_list:
            if share_file["dir"] and task.get("update_subdir", False):
                pattern, replace = task["update_subdir"], ""
            else:
                # å¦‚æœæ²¡æœ‰patternå’Œreplaceå­—æ®µï¼Œåˆ™åŒ¹é…æ‰€æœ‰æ–‡ä»¶
                if 'pattern' not in task:
                    pattern, replace = ".*", ""
                else:
                    pattern, replace = magic_regex_func(task["pattern"], task["replace"])
            if re.search(pattern, share_file["file_name"]):
                save_name = (
                    re.sub(pattern, replace, share_file["file_name"])
                    if replace != ""
                    else share_file["file_name"]
                )
            if task.get("ignore_extension") and not share_file["dir"]:
                def compare_func(a: str, b1: str, b2: str) -> bool:
                    return (os.path.splitext(a)[0] == os.path.splitext(b1)[0]
                            or os.path.splitext(a)[0] == os.path.splitext(b2)[0])
            else:
                def compare_func(a: str, b1: str, b2: str) -> bool:
                    return a == b1 or a == b2
                file_exists = any(
                    compare_func(
                        dir_file["file_name"], share_file["file_name"], save_name
                    )
                    for dir_file in dir_file_list
                )
                if not file_exists:
                    share_file["save_name"] = save_name
                    need_save_list.append(share_file)
                elif share_file["dir"]:
                    if task.get("update_subdir", False):
                        logger.info(f"æ£€æŸ¥å­æ–‡ä»¶å¤¹ï¼š{savepath}/{share_file['file_name']}")
                        subdir_tree = await self.dir_check_and_save(
                            session,
                            task,
                            pwd_id,
                            stoken,
                            share_file["fid"],
                            f"{subdir_path}/{share_file['file_name']}",
                        )
                        if subdir_tree.size(1) > 0:
                            tree.create_node(
                                "ğŸ“" + share_file["file_name"],
                                share_file["fid"],
                                parent=pdir_fid,
                            )
                            tree.merge(share_file["fid"], subdir_tree, deep=False)
            if share_file["fid"] == task.get("startfid", ""):
                break

        fid_list = [item["fid"] for item in need_save_list]
        fid_token_list = [item["share_fid_token"] for item in need_save_list]
        save_name_list = [item["save_name"] for item in need_save_list]
        if fid_list:
            save_file_return = await self.save_file(session, fid_list, fid_token_list, to_pdir_fid, pwd_id, stoken)
            err_msg = None
            if save_file_return and save_file_return.get("code") == 0:
                task_id = save_file_return["data"]["task_id"]
                query_task_return = await self.query_task(session, task_id)
                if query_task_return and query_task_return.get("code") == 0:
                    save_name_list.sort()
                    for item in need_save_list:
                        icon = (
                            "ğŸ“"
                            if item["dir"]
                            else "ğŸï¸" if item["obj_category"] == "video" else ""
                        )
                        tree.create_node(
                            f"{icon}{item['save_name']}", item["fid"], parent=pdir_fid
                        )
                else:
                    err_msg = query_task_return["message"] if query_task_return else "æ— å“åº”"
            else:
                err_msg = save_file_return["message"] if save_file_return else "æ— å“åº”"

            if err_msg:
                add_notify(f"âŒã€Š{task['taskname']}ã€‹è½¬å­˜å¤±è´¥ï¼š{err_msg}\n")
        return tree

    async def query_task(self, session: aiohttp.ClientSession, task_id: str) -> Optional[Dict[str, Any]]:
        retry_index = 0
        while True:
            url = "https://drive-pc.quark.cn/1/clouddrive/task"
            querystring = {
                "pr": "ucpro",
                "fr": "pc",
                "uc_param_str": "",
                "task_id": task_id,
                "retry_index": retry_index,
                "__dt": int(random.uniform(1, 5) * 60 * 1000),
                "__t": datetime.now().timestamp(),
            }
            headers = self.common_headers()
            response = await fetch(session, "GET", url, headers=headers, params=querystring)
            if response:
                if response["data"]["status"] != 0:
                    break
                else:
                    if retry_index == 0:
                        logger.info(f"æ­£åœ¨ç­‰å¾…[{response['data']['task_title']}]æ‰§è¡Œç»“æœ")
                    else:
                        logger.info(".")
                    retry_index += 1
                    await asyncio.sleep(0.5)
            else:
                break
        return response

    async def do_rename_task(self, session: aiohttp.ClientSession, task: Dict[str, Any], subdir_path: str = "") -> bool:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æœ‰patternå’Œreplaceå­—æ®µ
        if "pattern" not in task or "replace" not in task:
            # å¦‚æœæ²¡æœ‰patternå’Œreplaceå­—æ®µï¼Œè·³è¿‡é‡å‘½åä»»åŠ¡
            return False
        pattern, replace = magic_regex_func(task["pattern"], task["replace"])
        if not pattern or not replace:
            return False
        savepath = re.sub(r"/{2,}", "/", f"/{task['savepath']}{subdir_path}")
        if not self.savepath_fid.get(savepath):
            fids = await self.get_fids(session, (savepath,))
            if fids:
                self.savepath_fid[savepath] = fids[0]["fid"]
            else:
                return False
        dir_file_list = await self.ls_dir(session, self.savepath_fid[savepath])
        dir_file_name_list = [item["file_name"] for item in dir_file_list]
        rename_tasks = []
        for dir_file in dir_file_list:
            if dir_file["dir"]:
                rename_tasks.append(self.do_rename_task(session, task, f"{subdir_path}/{dir_file['file_name']}"))
            if re.search(pattern, dir_file["file_name"]):
                save_name = (
                    re.sub(pattern, replace, dir_file["file_name"])
                    if replace != ""
                    else dir_file["file_name"]
                )
                if save_name != dir_file["file_name"] and (
                    save_name not in dir_file_name_list
                ):
                    rename_tasks.append(self.rename(session, dir_file["fid"], save_name))
        rename_results = await asyncio.gather(*rename_tasks)
        is_rename = any(rename_results)
        return is_rename

async def verify_account(session: aiohttp.ClientSession, account: Quark) -> bool:
    logger.info(f"â–¶ï¸ éªŒè¯ç¬¬{account.index}ä¸ªè´¦å·")
    if "__uid" not in account.cookie:
        logger.info(f"ğŸ’¡ ä¸å­˜åœ¨cookieå¿…è¦å‚æ•°ï¼Œåˆ¤æ–­ä¸ºä»…ç­¾åˆ°")
        return False
    else:
        account_info = await account.init(session)
        if not account_info:
            add_notify(f"ğŸ‘¤ ç¬¬{account.index}ä¸ªè´¦å·ç™»å½•å¤±è´¥ï¼Œcookieæ— æ•ˆâŒ")
            return False
        else:
            logger.info(f"ğŸ‘¤ è´¦å·æ˜µç§°: {account_info['nickname']}âœ…")
            return True

def format_bytes(size_bytes: int) -> str:
    units = ("B", "KB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB")
    i = 0
    size_float = float(size_bytes)
    while size_float >= 1024 and i < len(units) - 1:
        size_float /= 1024
        i += 1
    return f"{size_float:.2f} {units[i]}"

async def do_sign(session: aiohttp.ClientSession, account: Quark) -> None:
    if not account.mparam:
        logger.info("â­ï¸ ç§»åŠ¨ç«¯å‚æ•°æœªè®¾ç½®ï¼Œè·³è¿‡ç­¾åˆ°")
        return
    growth_info = await account.get_growth_info(session)
    if growth_info and isinstance(growth_info, dict):
        # å®‰å…¨åœ°è®¿é—®å­—å…¸é”®
        is_88vip = growth_info.get('88VIP', False)
        total_capacity = growth_info.get('total_capacity', 0)
        cap_composition = growth_info.get('cap_composition', {})
        sign_reward = cap_composition.get('sign_reward', 0)
        
        growth_message = f"ğŸ’¾ {'88VIP' if is_88vip else 'æ™®é€šç”¨æˆ·'} æ€»ç©ºé—´ï¼š{format_bytes(total_capacity)}ï¼Œç­¾åˆ°ç´¯è®¡è·å¾—ï¼š{format_bytes(sign_reward)}"
        
        cap_sign = growth_info.get('cap_sign', {})
        if isinstance(cap_sign, dict) and cap_sign.get('sign_daily'):
            sign_daily_reward = cap_sign.get('sign_daily_reward', 0)
            sign_progress = int(cap_sign.get('sign_progress', 0))
            sign_target = int(cap_sign.get('sign_target', 0))
            
            # å®‰å…¨åœ°è¿›è¡Œé™¤æ³•è¿ç®—
            reward_mb = 0
            if sign_daily_reward:
                try:
                    reward_mb = int(sign_daily_reward / 1024 / 1024)
                except (TypeError, ValueError):
                    reward_mb = 0
            sign_message = f"ğŸ“… ç­¾åˆ°è®°å½•: ä»Šæ—¥å·²ç­¾åˆ°+{reward_mb}MBï¼Œè¿ç­¾è¿›åº¦({sign_progress}/{sign_target})âœ…"
            message = f"{sign_message}\n{growth_message}"
            logger.info(message)
        else:
            sign, sign_return = await account.get_growth_sign(session)
            if sign:
                sign_progress = int(cap_sign.get('sign_progress', 0))
                sign_target = int(cap_sign.get('sign_target', 0))
                
                # å®‰å…¨åœ°è¿›è¡Œé™¤æ³•è¿ç®—
                sign_return_mb = 0
                if isinstance(sign_return, (int, float)):
                    try:
                        sign_return_mb = int(sign_return / 1024 / 1024)
                    except (TypeError, ValueError):
                        sign_return_mb = 0
                sign_message = f"ğŸ“… æ‰§è¡Œç­¾åˆ°: ä»Šæ—¥ç­¾åˆ°+{sign_return_mb}MBï¼Œè¿ç­¾è¿›åº¦({sign_progress + 1}/{sign_target})âœ…"
                message = f"{sign_message}\n{growth_message}"
                
                # ç§»é™¤ç­¾åˆ°é€šçŸ¥åŠŸèƒ½ï¼Œåªè®°å½•æ—¥å¿—
                logger.info(message)
            else:
                logger.error(f"ğŸ“… ç­¾åˆ°å¼‚å¸¸: {sign_return}")

async def do_save(session: aiohttp.ClientSession, account: Quark, tasklist: List[Dict[str, Any]] = []) -> None:
    emby = Emby(
        CONFIG_DATA.get("emby", {}).get("url", ""),
        CONFIG_DATA.get("emby", {}).get("apikey", ""),
    )
    logger.info(f"è½¬å­˜è´¦å·: {account.nickname}")
    await account.update_savepath_fid(session, tasklist)

    def check_date(task):
        return (
            (not task.get("enddate") or datetime.now().date() <= datetime.strptime(task["enddate"], "%Y-%m-%d").date())
            and (
                not task.get("runweek")
                or (datetime.today().weekday() + 1 in task.get("runweek"))
            )
        )

    tasks = []
    for index, task in enumerate(tasklist):
        if check_date(task):
            logger.info(f"#{index+1}------------------")
            logger.info(f"ä»»åŠ¡åç§°: {task['taskname']}")
            logger.info(f"åˆ†äº«é“¾æ¥: {task['shareurl']}")
            logger.info(f"ç›®æ ‡ç›®å½•: {task['savepath']}")
            if 'pattern' in task:
                logger.info(f"æ­£åˆ™åŒ¹é…: {task['pattern']}")
            if 'replace' in task:
                logger.info(f"æ­£åˆ™æ›¿æ¢: {task['replace']}")
            if task.get("enddate"):
                logger.info(f"ä»»åŠ¡æˆªæ­¢: {task['enddate']}")
            if task.get("emby_id"):
                logger.info(f"åˆ·åª’ä½“åº“: {task['emby_id']}")
            if task.get("ignore_extension"):
                logger.info(f"å¿½ç•¥åç¼€: {task['ignore_extension']}")
            if task.get("update_subdir"):
                logger.info(f"æ›´å­ç›®å½•: {task['update_subdir']}")
            is_new = await account.do_save_task(session, task)
            is_rename = await account.do_rename_task(session, task)
            if emby.is_active and (is_new or is_rename) and task.get("emby_id") != "0":
                if task.get("emby_id"):
                    await emby.refresh(session, task["emby_id"])
                else:
                    match_emby_id = await emby.search(session, task["taskname"])
                    if match_emby_id:
                        task["emby_id"] = match_emby_id
                        await emby.refresh(session, match_emby_id)
    logger.info("è½¬å­˜ä»»åŠ¡å®Œæˆ")

class Emby:
    def __init__(self, emby_url: str, emby_apikey: str):
        self.is_active = False
        if emby_url and emby_apikey:
            self.emby_url = emby_url
            self.emby_apikey = emby_apikey
            # åˆå§‹åŒ–æ—¶ä¸è¿›è¡Œè¯·æ±‚ï¼Œéœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è°ƒç”¨æ–¹æ³•

    async def get_info(self, session):
        url = f"{self.emby_url}/emby/System/Info"
        headers = {"X-Emby-Token": self.emby_apikey}
        response = await fetch(session, "GET", url, headers=headers, params={})
        if response and "application/json" in response.get("Content-Type", ""):
            logger.info(
                f"Embyåª’ä½“åº“: {response.get('ServerName','')} v{response.get('Version','')}"
            )
            return True
        else:
            logger.error(f"Embyåª’ä½“åº“: è¿æ¥å¤±è´¥âŒ {response.get('text', 'æ— å“åº”') if response else 'æ— å“åº”'}")
            return False

    async def refresh(self, session, emby_id):
        if emby_id:
            url = f"{self.emby_url}/emby/Items/{emby_id}/Refresh"
            headers = {"X-Emby-Token": self.emby_apikey}
            querystring = {
                "Recursive": "true",
                "MetadataRefreshMode": "FullRefresh",
                "ImageRefreshMode": "FullRefresh",
                "ReplaceAllMetadata": "false",
                "ReplaceAllImages": "false",
            }
            response = await fetch(session, "POST", url, headers=headers, params=querystring)
            if response and response.get("text") == "":
                logger.info(f"ğŸ åˆ·æ–°Embyåª’ä½“åº“ï¼šæˆåŠŸâœ…")
                return True
            else:
                logger.error(f"ğŸ åˆ·æ–°Embyåª’ä½“åº“ï¼š{response.get('text', 'æ— å“åº”') if response else 'æ— å“åº”'}âŒ")
                return False

    async def search(self, session, media_name):
        if media_name:
            url = f"{self.emby_url}/emby/Items"
            headers = {"X-Emby-Token": self.emby_apikey}
            querystring = {
                "IncludeItemTypes": "Series",
                "StartIndex": 0,
                "SortBy": "SortName",
                "SortOrder": "Ascending",
                "ImageTypeLimit": 0,
                "Recursive": "true",
                "SearchTerm": media_name,
                "Limit": 10,
                "IncludeSearchTypes": "false",
            }
            response = await fetch(session, "GET", url, headers=headers, params=querystring)
            if response and "application/json" in response.get("Content-Type", ""):
                if response.get("Items"):
                    for item in response["Items"]:
                        if item["IsFolder"]:
                            logger.info(
                                f"ğŸ ã€Š{item['Name']}ã€‹åŒ¹é…åˆ°Embyåª’ä½“åº“IDï¼š{item['Id']}"
                            )
                            return item["Id"]
            else:
                logger.error(f"ğŸ æœç´¢Embyåª’ä½“åº“ï¼š{response.get('text', 'æ— å“åº”') if response else 'æ— å“åº”'}âŒ")
        return False

async def main():
    global CONFIG_DATA
    start_time = datetime.now()
    logger.info("===============ç¨‹åºå¼€å§‹===============")
    logger.info(f"â° æ‰§è¡Œæ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    config_path = sys.argv[1] if len(sys.argv) > 1 else "quark_config.json"
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    task_index = None
    cookie_index = None
    
    if len(sys.argv) > 2:
        # æ£€æŸ¥ç¬¬äºŒä¸ªå‚æ•°æ˜¯å¦æ˜¯æ•°å­—ï¼ˆå¯èƒ½æ˜¯task_indexæˆ–cookie_indexï¼‰
        if sys.argv[2].isdigit():
            # å½“Webç•Œé¢è°ƒç”¨æ—¶ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯cookie_index
            # å½“å‘½ä»¤è¡Œè°ƒç”¨æ—¶ï¼Œå¯èƒ½æ˜¯task_index
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å‡è®¾å½“åªæœ‰ä¸¤ä¸ªå‚æ•°æ—¶ï¼Œç¬¬äºŒä¸ªæ˜¯cookie_index
            # å½“æœ‰ä¸‰ä¸ªå‚æ•°æ—¶ï¼Œç¬¬äºŒä¸ªæ˜¯task_indexï¼Œç¬¬ä¸‰ä¸ªæ˜¯cookie_index
            if len(sys.argv) == 3:
                # åªæœ‰ä¸¤ä¸ªå‚æ•°ï¼šconfig.json cookie_index
                cookie_index = int(sys.argv[2])
            elif len(sys.argv) > 3:
                # æœ‰ä¸‰ä¸ªæˆ–æ›´å¤šå‚æ•°ï¼šconfig.json task_index cookie_index
                task_index = int(sys.argv[2])
                if sys.argv[3].isdigit():
                    cookie_index = int(sys.argv[3])

    if not os.path.exists(config_path):
        if os.environ.get("QUARK_COOKIE"):
            logger.info(
                f"âš™ï¸ è¯»å–åˆ° QUARK_COOKIE ç¯å¢ƒå˜é‡ï¼Œä»…ç­¾åˆ°é¢†ç©ºé—´ã€‚å¦‚éœ€æ‰§è¡Œè½¬å­˜ï¼Œè¯·åˆ é™¤è¯¥ç¯å¢ƒå˜é‡åé…ç½® {config_path} æ–‡ä»¶"
            )
            cookie_val = os.environ.get("QUARK_COOKIE")
            cookie_form_file = False
        else:
            logger.info(f"âš™ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨âŒï¼Œæ­£è¿œç¨‹ä»ä¸‹è½½é…ç½®æ¨¡ç‰ˆ")
            config_url = f"{GH_PROXY}https://raw.githubusercontent.com/Cp0204/quark_auto_save/main/quark_config.json"
            if download_file_sync(config_url, config_path):
                logger.info("âš™ï¸ é…ç½®æ¨¡ç‰ˆä¸‹è½½æˆåŠŸâœ…ï¼Œè¯·åˆ°ç¨‹åºç›®å½•ä¸­æ‰‹åŠ¨é…ç½®")
            return
    else:
        logger.info(f"âš™ï¸ æ­£ä» {config_path} æ–‡ä»¶ä¸­è¯»å–é…ç½®")
        with open(config_path, "r", encoding="utf-8") as file:
            CONFIG_DATA = json.load(file)
        if not CONFIG_DATA.get("magic_regex"):
            CONFIG_DATA["magic_regex"] = MAGIC_REGEX
        cookie_form_file = True

    # æ”¯æŒæ–°çš„cookiesæ•°ç»„ç»“æ„
    if "cookies" in CONFIG_DATA:
        # æ–°ç»“æ„ï¼šcookiesæ•°ç»„ï¼Œæ¯ä¸ªcookieæœ‰è‡ªå·±çš„tasklist
        cookies_data = CONFIG_DATA["cookies"]
        cookies = [cookie_data["cookie"] for cookie_data in cookies_data]
        cookie_names = [cookie_data["name"] for cookie_data in cookies_data]
        cookie_tasklists = [cookie_data.get("tasklist", []) for cookie_data in cookies_data]
    else:
        # æ—§ç»“æ„ï¼šå…¼å®¹å¤„ç†
        cookie_val = CONFIG_DATA.get("cookie")
        cookies_result = get_cookies(cookie_val)
        if isinstance(cookies_result, list):
            cookies = cookies_result
            cookie_names = [f"è´¦å·{i+1}" for i in range(len(cookies))]
            cookie_tasklists = [CONFIG_DATA.get("tasklist", [])] * len(cookies)
        else:
            cookies = []
            cookie_names = []
            cookie_tasklists = []

    if not cookies:
        logger.error("âŒ cookie æœªé…ç½®")
        return

    async with aiohttp.ClientSession() as session:
        accounts = [Quark(cookie, index) for index, cookie in enumerate(cookies)]
        logger.info("===============éªŒè¯è´¦å·===============")
        verify_tasks = [verify_account(session, account) for account in accounts]
        await asyncio.gather(*verify_tasks)
        logger.info("===============ç­¾åˆ°ä»»åŠ¡===============")
        sign_tasks = [do_sign(session, account) for account in accounts]
        await asyncio.gather(*sign_tasks)
        logger.info("===============è½¬å­˜ä»»åŠ¡===============")
        
        # ä¸ºæ¯ä¸ªæœ‰æ•ˆçš„è´¦å·æ‰§è¡Œå¯¹åº”çš„è½¬å­˜ä»»åŠ¡
        for i, account in enumerate(accounts):
            if account.is_active and cookie_form_file and i < len(cookie_tasklists):
                tasklist = cookie_tasklists[i]
                if tasklist:  # åªæœ‰è¯¥cookieæœ‰ä»»åŠ¡æ—¶æ‰æ‰§è¡Œè½¬å­˜
                    # å¦‚æœæŒ‡å®šäº†cookie_indexï¼Œåªå¤„ç†è¯¥cookie
                    if cookie_index is not None and i != cookie_index:
                        continue
                    
                    logger.info(f"===============å¤„ç†è´¦å·: {cookie_names[i]} ===============")
                    if task_index is not None and 0 <= task_index < len(tasklist):
                        await do_save(session, account, [tasklist[task_index]])
                    else:
                        await do_save(session, account, tasklist)
                    
                    # å¤„ç†å®Œå½“å‰è´¦å·åï¼Œå‘é€è¯¥è´¦å·çš„é€šçŸ¥
                    if NOTIFYS:
                        notify_body = "\n".join(NOTIFYS)
                        send_ql_notify("ã€å¤¸å…‹è‡ªåŠ¨è¿½æ›´ã€‘", notify_body, cookie_index=i)
                        # æ¸…ç©ºNOTIFYSï¼Œä¸ºä¸‹ä¸€ä¸ªè´¦å·åšå‡†å¤‡
                        NOTIFYS.clear()
        
        logger.info("===============æ¨é€é€šçŸ¥===============")
        # è¿™é‡Œä¸å†éœ€è¦å‘é€é€šçŸ¥ï¼Œå› ä¸ºæ¯ä¸ªè´¦å·å¤„ç†å®Œåå·²ç»å‘é€äº†
        if cookie_form_file:
            with open(config_path, "w", encoding="utf-8") as file:
                json.dump(CONFIG_DATA, file, ensure_ascii=False, indent=2)
    end_time = datetime.now()
    duration = end_time - start_time
    logger.info("===============ç¨‹åºç»“æŸ===============")
    logger.info(f"ğŸ˜ƒ è¿è¡Œæ—¶é•¿: {round(duration.total_seconds(), 2)}s")

if __name__ == "__main__":
    asyncio.run(main())
